{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606dbe3",
   "metadata": {
    "id": "2606dbe3"
   },
   "outputs": [],
   "source": [
    "# Import neded packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "# #%matplotlib widget\n",
    "# %matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12099a17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12099a17",
    "outputId": "f32b0a08-85e5-48c4-dc1f-79fd4fc3ff06"
   },
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H9QXRTwlqTEL",
   "metadata": {
    "id": "H9QXRTwlqTEL"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1f7fb",
   "metadata": {
    "id": "10b1f7fb"
   },
   "outputs": [],
   "source": [
    "# Defined the DGM network\n",
    "\n",
    "\n",
    "#__________________________ The class that defines the DGM layer ______________________________\n",
    "\n",
    "class DGM_layer(nn.Module):\n",
    "    \"\"\"\n",
    "        The parametres:\n",
    "                        d: dimension of the space domain\n",
    "                        M: number of units in each layer\n",
    "                  \"\"\"\n",
    "\n",
    "    def __init__(self,d,M):\n",
    "        super(DGM_layer, self).__init__()\n",
    "        self.Uz = nn.Linear(d, M, bias=False)\n",
    "        self.Wzbz = nn.Linear(M, M)\n",
    "        self.Ug = nn.Linear(d, M, bias=False)\n",
    "        self.Wgbg = nn.Linear(M, M)\n",
    "        self.Ur = nn.Linear(d, M, bias=False)\n",
    "        self.Wrbr = nn.Linear(M, M)\n",
    "        self.Uh = nn.Linear(d, M, bias=False)\n",
    "        self.Whbh = nn.Linear(M, M)\n",
    "        self.onesTens = torch.ones(M).to(device)\n",
    "\n",
    "    def activation(self, x):\n",
    "        return torch.tanh(x)\n",
    "        #return torch.sigmoid(x)\n",
    "        #return x * torch.sigmoid(x)\n",
    "        #return torch.relu(x)\n",
    "        #return torch.cos(x)\n",
    "\n",
    "    def forward(self, xt, prevS):\n",
    "        Z = self.activation(self.Uz(xt) + self.Wzbz(prevS))\n",
    "        G = self.activation(self.Ug(xt) + self.Wgbg(prevS))\n",
    "        R = self.activation(self.Ur(xt) + self.Wrbr(prevS))\n",
    "        SR = prevS * R\n",
    "        H = self.activation(self.Uh(xt) + self.Whbh(SR))\n",
    "        return (self.onesTens - G) * H + Z * prevS\n",
    "\n",
    "\n",
    "#__________________________ The class that defines the DGM network ______________________________\n",
    "\n",
    "\n",
    "class DGM_Net(nn.Module):\n",
    "    \"\"\"\n",
    "        The parametres:\n",
    "                        d: dimension of the space domain\n",
    "                        M: number of units in each layer\n",
    "                        L: number of DGM layers\n",
    "                        X: the vector of  spatial data\n",
    "                        t: the vector of time data\n",
    "                  \"\"\"\n",
    "\n",
    "    def __init__(self, d, M, L):\n",
    "        super(DGM_Net, self).__init__()\n",
    "        self.initial_layer = nn.Linear(d, M)\n",
    "        self.middle_layers = nn.ModuleList([DGM_layer(d, M) for i in range(L)])\n",
    "        self.final_layer = nn.Linear(M, 1)\n",
    "\n",
    "    def activation(self, x):\n",
    "        return torch.tanh(x)\n",
    "        #return torch.sigmoid(x)\n",
    "        #return x * torch.sigmoid(x)\n",
    "        #return torch.relu(x)\n",
    "        #return torch.cos(x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        S = self.activation(self.initial_layer(X))\n",
    "        for i, DGMlayer in enumerate(self.middle_layers):\n",
    "            S = DGMlayer(X, S)\n",
    "        return self.final_layer(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c7431",
   "metadata": {
    "id": "f59c7431"
   },
   "outputs": [],
   "source": [
    "#_______________ hyperparameters __________________________________\n",
    "\n",
    "dim = 2                          # imension of the space domain\n",
    "M = 20                           # number of units in each layer\n",
    "L = 3                            # number of DGM layers\n",
    "num_eps = 50000                  # number of epochs totale\n",
    "batch_size = 1024\n",
    "\n",
    "mini_batch_size_bdry = 250\n",
    "x_low = -1.0\n",
    "x_high = 1.0               # domain dimensions\n",
    "T = 1.0\n",
    "\n",
    "alpha = 0.8\n",
    "betha = 0.4\n",
    "\n",
    "area = np.abs(x_high - x_low)**dim    # domain measure for using the Monte Carlo approximation\n",
    "\n",
    "num_parts = 8                           # To choose a size of mini patch\n",
    "mini_batch_size = batch_size // num_parts     # Calculate the size of mini patch\n",
    "\n",
    "\n",
    "# used in calculating boundary loss\n",
    "ones = torch.ones((mini_batch_size_bdry, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81be48",
   "metadata": {
    "id": "1b81be48"
   },
   "outputs": [],
   "source": [
    "#___________________ the analytical solution __________________________________________________________\n",
    "\n",
    "pi= np.pi\n",
    "def U_exat(X):\n",
    "    t = X[:,0].reshape(-1,1)\n",
    "    x = X[:,1].reshape(-1,1)\n",
    "    y = X[:,2].reshape(-1,1)\n",
    "    return torch.exp(t)*torch.sin(pi*x)*torch.sin(pi*y)\n",
    "\n",
    "#___________________ the second member ________________________________________________________________\n",
    "\n",
    "def f(X):\n",
    "    return (1+2*pi**2)*U_exat(X)\n",
    "\n",
    "#___________________ Initial condition ________________________________________________________________\n",
    "\n",
    "def h(X):\n",
    "    x = X[:,1].reshape(-1,1)\n",
    "    y = X[:,2].reshape(-1,1)\n",
    "    return (torch.sin(pi*x)*torch.sin(pi*y)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8bf6e2",
   "metadata": {
    "id": "ee8bf6e2"
   },
   "outputs": [],
   "source": [
    "\n",
    "   # the probability density function of the target distribution of probability\n",
    "def target_proba(u, f, x, alpha):\n",
    "\n",
    "    \"\"\"\n",
    "    The parametres:\n",
    "                        u: the neural network\n",
    "                        f: the second member of the PDE\n",
    "                        X: the current point to evaluate\n",
    "                                                            \"\"\"\n",
    "    x.requires_grad_()\n",
    "    u_output = u(x)\n",
    "    u_grad = torch.autograd.grad(u_output, x, grad_outputs=torch.ones_like(u_output), create_graph=True)[0]\n",
    "    u_t = (u_grad[:,0]).reshape(-1,1)\n",
    "    Δu = 0.0\n",
    "    for i in range(1,dim+1):\n",
    "        Δu += (torch.autograd.grad(u_grad[:,i], x, grad_outputs=torch.ones_like(u_grad[:,i]), create_graph=True)[0][:,i]).reshape(-1,1)\n",
    "\n",
    "\n",
    "    P_x = (torch.abs( u_t-Δu - f(x)))**(alpha)\n",
    "    P_sum = P_x.sum()\n",
    "    return P_x/P_sum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96697ac",
   "metadata": {
    "id": "e96697ac"
   },
   "outputs": [],
   "source": [
    "#_______________ Mean errors ____________________________________\n",
    "\n",
    "def absulat_ME(Uext,Upred):\n",
    "    return  torch.sqrt(((Uext - Upred)**2).mean())\n",
    "\n",
    "def relative_E(Uext,Upred):\n",
    "    return torch.sqrt(((Uext - Upred)**2).mean()/((Uext)**2).mean())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MzKrRQh_JTCM",
   "metadata": {
    "id": "MzKrRQh_JTCM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0141f3a",
   "metadata": {
    "id": "b0141f3a"
   },
   "outputs": [],
   "source": [
    "# Define the learning rate schedule based on the number of epochs\n",
    "def lr_lambda(epoch):\n",
    "    if epoch <= 5000:\n",
    "        return 1e-2\n",
    "    elif 5000 < epoch <= 10000:\n",
    "        return 5e-3\n",
    "    elif 10000 < epoch <= 20000:\n",
    "        return 1e-3\n",
    "    elif 20000 < epoch <= 30000:\n",
    "        return 5e-4\n",
    "    elif 30000 < epoch <= 40000:\n",
    "        return 1e-4\n",
    "    elif 40000 < epoch <= 45000:\n",
    "        return 5e-5\n",
    "    else:\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8a9a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6f8a9a5",
    "outputId": "fd7fab10-6752-4e73-de13-4fb635cbf650",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#________________________ Training __________________________________________________________________________\n",
    "\n",
    "u = DGM_Net(dim+1,M,L).to(device)   #  The network that will approach the solution\n",
    "\n",
    "\n",
    "x_sampler = torch.distributions.uniform.Uniform(x_low, x_high)    #   to obtain the spatial data\n",
    "t_sampler = torch.distributions.uniform.Uniform(0, T)    #   to obtain the temporal data\n",
    "\n",
    "\n",
    "# Create the optimizer and a LambdaLR scheduler with the defined learning rate schedule\n",
    "optimizer = optim.Adam(u.parameters(),lr = 1)                   # Define the optimizer\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)  # to adjust learning rate\n",
    "\n",
    "\n",
    "loss_train = np.zeros(num_eps)             # loss of taining inside the domaine\n",
    "relative_E_losses = np.zeros(num_eps)              # losses initialization\n",
    "\n",
    "print('\\n Using {} device'.format(device))\n",
    "for ep in range(num_eps):\n",
    "\n",
    "    #______________Sampling point according the distribution P__________________\n",
    "    u_sampling = u\n",
    "    XY = x_sampler.sample((100*batch_size, dim))\n",
    "    t = t_sampler.sample((100*batch_size, 1))\n",
    "    XY = torch.cat((t,XY),1)\n",
    "    XY = XY.to(device)\n",
    "\n",
    "    P = target_proba(u, f, XY, alpha).detach().cpu().squeeze()\n",
    "    m = torch.distributions.categorical.Categorical(P)\n",
    "    XY_indices = m.sample([batch_size,1])\n",
    "    x = XY[XY_indices].squeeze()\n",
    "\n",
    "    #_____________Compute importance-sampling weight____________________________\n",
    "    P_XY = P[XY_indices].squeeze()\n",
    "    W_xy = (batch_size*P_XY)**(-betha)\n",
    "    W_XY = torch.tensor(W_xy / W_xy.max()).to(device)\n",
    "\n",
    "\n",
    "    #______________Divide the tensor into equal parts___________________________\n",
    "    X_i = [x[i * mini_batch_size : (i + 1) * mini_batch_size] for i in range(num_parts)]\n",
    "    W_XY_i = [W_XY[i * mini_batch_size : (i + 1) * mini_batch_size] for i in range(num_parts)]\n",
    "\n",
    "\n",
    "    x_bndr = x_sampler.sample((mini_batch_size_bdry, 1))\n",
    "    t_bndr = t_sampler.sample((mini_batch_size_bdry, 1))\n",
    "    x_0 = x_sampler.sample((mini_batch_size, dim))\n",
    "    t_0 = torch.zeros((mini_batch_size, 1))\n",
    "\n",
    "    for w_XY_i, x_i in zip(W_XY_i,X_i):\n",
    "        # evaluate forward pass, compute derivatives of network with respect to x\n",
    "        #x_i = torch.Tensor(x_i).to(device)\n",
    "        #x_i.requires_grad_()\n",
    "        u_output = u(x_i)\n",
    "        u_grad = torch.autograd.grad(u_output, x_i, grad_outputs=torch.ones_like(u_output), create_graph=True)[0]\n",
    "        u_t = (u_grad[:,0]).reshape(-1,1)\n",
    "        Δu = 0.0\n",
    "        for i in range(1,dim+1):\n",
    "            Δu += (torch.autograd.grad(u_grad[:,i], x_i, grad_outputs=torch.ones_like(u_grad[:,i]), create_graph=True)[0][:,i]).reshape(-1,1)\n",
    "\n",
    "        L1 = torch.mean(w_XY_i * torch.pow((u_t-Δu-f(x_i)), 2))\n",
    "\n",
    "\n",
    "\n",
    "        x_bry_1 = torch.cat((t_bndr,-ones,x_bndr),1).to(device)\n",
    "        L2 = torch.mean(torch.pow(u(x_bry_1), 2))\n",
    "        x_bry_2 = torch.cat((t_bndr,ones,x_bndr),1).to(device)\n",
    "        L3 = torch.mean(torch.pow(u(x_bry_2), 2))                     # Boundary conditions\n",
    "        y_bry_1 = torch.cat((t_bndr,x_bndr,-ones),1).to(device)\n",
    "        L4 = torch.mean(torch.pow(u(y_bry_1), 2))\n",
    "        y_bry_2 = torch.cat((t_bndr,x_bndr,ones),1).to(device)\n",
    "        L5 = torch.mean(torch.pow(u(y_bry_2), 2))\n",
    "\n",
    "\n",
    "        xt_0 = torch.cat((t_0,x_0),1)\n",
    "        xt_0 = torch.Tensor(xt_0).to(device)\n",
    "        xt_0.requires_grad_()\n",
    "\n",
    "        L6 = torch.mean(torch.pow((u(xt_0) - h(xt_0)), 2))          # Initial conditions\n",
    "\n",
    "\n",
    "\n",
    "        Loss = L1 + L2 + L3 + L4 + L5 + L6\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward(retain_graph=True)            # compute derivative of loss with respect to network parameters\n",
    "        optimizer.step()           # update network parameters with ADAM\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step(ep)  # Pass the current epoch to the scheduler\n",
    "\n",
    "    # Display the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    relative_E_losses[ep] =  float(relative_E( U_exat(x) ,u(x)).item())\n",
    "    loss_train[ep] = float(L1.item())\n",
    "    if ep < 50:\n",
    "        print(\"Epoch: %d - L_r:%f  loss_int : %.2f - REN : %.2f\"%(ep, current_lr, loss_train[ep], relative_E_losses[ep]),\"%\")\n",
    "    elif ep % 1000 == 999:\n",
    "        betha = betha + (1000/num_eps)\n",
    "        print(\"Epoch: %d | L_r:%f | beta: %f | loss_int : %.2f | REN : %.2f\"%(ep, current_lr,betha, loss_train[ep], relative_E_losses[ep]),\"%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xLgITiy7sk2Y",
   "metadata": {
    "id": "xLgITiy7sk2Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mfI2JPN2l-Gi",
   "metadata": {
    "id": "mfI2JPN2l-Gi",
    "outputId": "add45825-1d04-4147-d332-90b9782ea8f8"
   },
   "outputs": [],
   "source": [
    "\n",
    "#_______________________________________________________________________________\n",
    "\n",
    "grid_nums = 500\n",
    "x_test = torch.distributions.uniform.Uniform(x_low, x_high).sample((grid_nums, dim)).to(device)\n",
    "t_test = torch.distributions.uniform.Uniform(0, T).sample((grid_nums, 1)).to(device)\n",
    "X_test = torch.cat((t_test,x_test),1)\n",
    "with torch.no_grad():\n",
    "    U = u(X_test).detach()\n",
    "\n",
    "U_ext =  U_exat(X_test)\n",
    "\n",
    "print(\"The Mean Absulat Error: {}\".format(absulat_ME(U_ext,U)))\n",
    "print(\"The Relative Error: {} %\".format(relative_E(U_ext,U)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842bbad",
   "metadata": {
    "id": "5842bbad",
    "outputId": "414837a3-7ed7-44c3-8d77-578c43ec650e"
   },
   "outputs": [],
   "source": [
    "MSEabs = 0\n",
    "MSErlf = 0\n",
    "N = 100\n",
    "for compt in range(N):\n",
    "    x_test = torch.distributions.uniform.Uniform(x_low, x_high).sample((grid_nums, dim)).to(device)\n",
    "    t_test = torch.distributions.uniform.Uniform(0, T).sample((grid_nums, 1)).to(device)\n",
    "    X_test = torch.cat((t_test,x_test),1)\n",
    "    U = u(X_test)\n",
    "    U_ext =  U_exat(X_test)\n",
    "\n",
    "    MSEabs+= absulat_ME(U_ext,U)\n",
    "    MSErlf+= relative_E(U_ext,U)\n",
    "\n",
    "print(\"The averag of Mean Absulat Error: {}\".format(MSEabs/N))\n",
    "print(\"The averag of Relative Error: {} %\".format(MSErlf/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbaeb03",
   "metadata": {
    "id": "bbbaeb03",
    "outputId": "adbe406e-cb5e-4307-c185-391d13ac098e"
   },
   "outputs": [],
   "source": [
    "#_______________ Ploting results _______________________________________________\n",
    "\n",
    "t_now = 0\n",
    "grid_nums = 50\n",
    "\n",
    "x_grid = torch.linspace(-1.0,1.0,grid_nums)\n",
    "XX, YY = torch.meshgrid(x_grid,x_grid)\n",
    "X = torch.reshape(XX, (-1,1))\n",
    "Y = torch.reshape(YY, (-1,1))\n",
    "\n",
    "t = t_now*torch.ones_like(X)\n",
    "XY = torch.cat((t,X,Y),1).to(device)\n",
    "with torch.no_grad():\n",
    "    U = u(XY)\n",
    "\n",
    "UU = torch.reshape(U, (grid_nums,grid_nums)).detach().cpu().numpy()\n",
    "U_ext =  U_exat(XY)\n",
    "U_ext = torch.reshape(U_ext , (grid_nums,grid_nums)).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Create the figure and axes\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "\n",
    "Z = U_ext - UU\n",
    "\n",
    "\n",
    "# =============\n",
    "# First subplot\n",
    "# =============\n",
    "# set up the axes for the first plot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(XX.numpy(), YY.numpy(), Z, cmap='seismic',\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "# fig.colorbar(surf)\n",
    "\n",
    "\n",
    "# ==============\n",
    "# Second subplot\n",
    "# ==============\n",
    "# set up the axes for the second plot\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "#Plot the filled contour plot\n",
    "contour = ax.contourf(XX.numpy(),YY.numpy(), Z,levels= 100,cmap='seismic',\n",
    "                      linewidth=0, antialiased=False)\n",
    "fig.colorbar(contour)\n",
    "# Add a colorbar for the contour plot\n",
    "# fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m1DmFmqkl97R",
   "metadata": {
    "id": "m1DmFmqkl97R"
   },
   "outputs": [],
   "source": [
    "# #_______________________ Save Model _____________________________________\n",
    "\n",
    "# torch.save(u.state_dict(), \"Model_A_DGM_Heat_equation\") # to save weights\n",
    "# np.save(\"loss_A_DGM_Heat_equation\" ,Losse)                # to save Losse\n",
    "# np.save(\"loss_resudPDE_A_DGM_Heat_equation\" ,Losse_resud)                # to save Losse resudel PDE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f78d595",
   "metadata": {
    "id": "0f78d595"
   },
   "outputs": [],
   "source": [
    "# # #_______________________ load Model _____________________________________\n",
    "\n",
    "# u_loaded = DGM_Net(dim,M,L).to(device)\n",
    "# state_dict = torch.load(\"/content/Model_HM_determinist_DGM_Poisson_special_10000.pth\")\n",
    "# u_loaded.load_state_dict(state_dict)\n",
    "# losse_standard = np.load(\"/content/loss_HM_determinist_DGM_Poisson_special_10000.pnry.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee5a6b",
   "metadata": {
    "id": "79ee5a6b"
   },
   "outputs": [],
   "source": [
    "#_______________ Ploting results __________________________________\n",
    "\n",
    "\n",
    "# grid_nums = 50\n",
    "\n",
    "# x_grid = torch.linspace(-1.0,1.0,grid_nums)\n",
    "# XX, YY = torch.meshgrid(x_grid,x_grid)\n",
    "# X = torch.reshape(XX, (-1,1))\n",
    "# Y = torch.reshape(YY, (-1,1))\n",
    "# XY = torch.cat((X,Y),1).to(device)\n",
    "# with torch.no_grad():\n",
    "#     U = u_loaded(XY)\n",
    "\n",
    "# UU = torch.reshape(U, (grid_nums,grid_nums)).detach().cpu().numpy()\n",
    "# U_ext =  U_exat(XY)\n",
    "# U_ext = torch.reshape(U_ext , (grid_nums,grid_nums)).detach().cpu().numpy()\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "# ax.plot_surface(XX.numpy(),YY.numpy(), UU )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab6f89",
   "metadata": {
    "id": "07ab6f89"
   },
   "outputs": [],
   "source": [
    "# Losse_HM  = losses.reshape(-1,1)\n",
    "# Losse_standard  = losse_standard.reshape(-1,1)\n",
    "\n",
    "# x_error = np.linspace(0,num_eps, num_eps)\n",
    "# plt.plot(x_error, Losse_HM)\n",
    "# plt.plot(x_error, Losse_standard)\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb1a55",
   "metadata": {
    "id": "c1cb1a55"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b12bb",
   "metadata": {
    "id": "536b12bb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
