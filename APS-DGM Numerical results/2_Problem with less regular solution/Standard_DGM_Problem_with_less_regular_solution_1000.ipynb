{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606dbe3",
   "metadata": {
    "id": "2606dbe3"
   },
   "outputs": [],
   "source": [
    "# Import neded packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "# #%matplotlib widget\n",
    "# %matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12099a17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12099a17",
    "outputId": "b911683e-b9c8-49fc-9c7d-2f89ec9555ac"
   },
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1f7fb",
   "metadata": {
    "id": "10b1f7fb"
   },
   "outputs": [],
   "source": [
    "# Defined the DGM network\n",
    "\n",
    "\n",
    "#__________________________ The class that defines the DGM layer ______________________________\n",
    "\n",
    "class DGM_layer(nn.Module):\n",
    "    \"\"\"\n",
    "        The parametres:\n",
    "                        d: dimension of the space domain\n",
    "                        M: number of units in each layer\n",
    "                  \"\"\"\n",
    "\n",
    "    def __init__(self,d,M):\n",
    "        super(DGM_layer, self).__init__()\n",
    "        self.Uz = nn.Linear(d, M, bias=False)\n",
    "        self.Wzbz = nn.Linear(M, M)\n",
    "        self.Ug = nn.Linear(d, M, bias=False)\n",
    "        self.Wgbg = nn.Linear(M, M)\n",
    "        self.Ur = nn.Linear(d, M, bias=False)\n",
    "        self.Wrbr = nn.Linear(M, M)\n",
    "        self.Uh = nn.Linear(d, M, bias=False)\n",
    "        self.Whbh = nn.Linear(M, M)\n",
    "        self.onesTens = torch.ones(M).to(device)\n",
    "\n",
    "    def activation(self, x):\n",
    "        return torch.tanh(x)\n",
    "        #return torch.sigmoid(x)\n",
    "        #return x * torch.sigmoid(x)\n",
    "        #return torch.relu(x)\n",
    "        #return torch.cos(x)\n",
    "\n",
    "    def forward(self, xt, prevS):\n",
    "        Z = self.activation(self.Uz(xt) + self.Wzbz(prevS))\n",
    "        G = self.activation(self.Ug(xt) + self.Wgbg(prevS))\n",
    "        R = self.activation(self.Ur(xt) + self.Wrbr(prevS))\n",
    "        SR = prevS * R\n",
    "        H = self.activation(self.Uh(xt) + self.Whbh(SR))\n",
    "        return (self.onesTens - G) * H + Z * prevS\n",
    "\n",
    "\n",
    "#__________________________ The class that defines the DGM network ______________________________\n",
    "\n",
    "\n",
    "class DGM_Net(nn.Module):\n",
    "    \"\"\"\n",
    "        The parametres:\n",
    "                        d: dimension of the space domain\n",
    "                        M: number of units in each layer\n",
    "                        L: number of DGM layers\n",
    "                        X: the vector of  spatial data\n",
    "                        t: the vector of time data\n",
    "                  \"\"\"\n",
    "\n",
    "    def __init__(self, d, M, L):\n",
    "        super(DGM_Net, self).__init__()\n",
    "        self.initial_layer = nn.Linear(d, M)\n",
    "        self.middle_layers = nn.ModuleList([DGM_layer(d, M) for i in range(L)])\n",
    "        self.final_layer = nn.Linear(M, 1)\n",
    "\n",
    "    def activation(self, x):\n",
    "        return torch.tanh(x)\n",
    "        #return torch.sigmoid(x)\n",
    "        #return x * torch.sigmoid(x)\n",
    "        #return torch.relu(x)\n",
    "        #return torch.cos(x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        S = self.activation(self.initial_layer(X))\n",
    "        for i, DGMlayer in enumerate(self.middle_layers):\n",
    "            S = DGMlayer(X, S)\n",
    "        return self.final_layer(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c7431",
   "metadata": {
    "id": "f59c7431"
   },
   "outputs": [],
   "source": [
    "#_______________ hyperparameters __________________________________\n",
    "\n",
    "dim = 2                          # imension of the space domain\n",
    "M = 20                           # number of units in each layer\n",
    "L = 3                            # number of DGM layers\n",
    "num_eps = 50000                  # number of epochs totale\n",
    "\n",
    "mini_batch_size = 1000\n",
    "\n",
    "mini_batch_size_bdry = 250\n",
    "x_low =  0.0\n",
    "x_high = 1.0               # domain dimensions\n",
    "pi = np.pi\n",
    "\n",
    "area = np.abs(x_high - x_low)**dim    # domain measure for using the Monte Carlo approximation\n",
    "\n",
    "\n",
    "# used in calculating boundary loss\n",
    "ones = torch.ones((mini_batch_size_bdry, 1))\n",
    "zeros = torch.zeros((mini_batch_size_bdry, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81be48",
   "metadata": {
    "id": "1b81be48"
   },
   "outputs": [],
   "source": [
    "#___________________ the analytical solution __________________________________________________________\n",
    "\n",
    "def U_exat(X):\n",
    "    x = X[:,0].reshape(-1,1)\n",
    "    y = X[:,1].reshape(-1,1)\n",
    "    return torch.cos(pi*x**(2.5)) + torch.cos(pi*y**(2.5))\n",
    "\n",
    "#___________________ the second member ________________________________________________________________\n",
    "\n",
    "def f(x):\n",
    "    u_output = U_exat(x)\n",
    "    u_grad = torch.autograd.grad(u_output, x, grad_outputs=torch.ones_like(u_output), create_graph=True)[0]\n",
    "    Δu = 0.0\n",
    "    for i in range(dim):\n",
    "        Δu += (torch.autograd.grad(u_grad[:,i], x, grad_outputs=torch.ones_like(u_grad[:,i]), create_graph=True)[0][:,i]).reshape(-1,1)\n",
    "    return -Δu + pi**2 * u_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96697ac",
   "metadata": {
    "id": "e96697ac"
   },
   "outputs": [],
   "source": [
    "#_______________ Mean errors ____________________________________\n",
    "\n",
    "def absulat_ME(Uext,Upred):\n",
    "    return  torch.sqrt(((Uext - Upred)**2).mean())\n",
    "\n",
    "def relative_E(Uext,Upred):\n",
    "    return torch.sqrt(((Uext - Upred)**2).mean()/((Uext)**2).mean())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e4062",
   "metadata": {
    "id": "de3e4062"
   },
   "outputs": [],
   "source": [
    "# Define the learning rate schedule based on the number of epochs\n",
    "def lr_lambda(epoch):\n",
    "    if epoch <= 5000:\n",
    "        return 1e-2\n",
    "    elif 5000 < epoch <= 10000:\n",
    "        return 5e-3\n",
    "    elif 10000 < epoch <= 20000:\n",
    "        return 1e-3\n",
    "    elif 20000 < epoch <= 30000:\n",
    "        return 5e-4\n",
    "    elif 30000 < epoch <= 40000:\n",
    "        return 1e-4\n",
    "    elif 40000 < epoch <= 45000:\n",
    "        return 5e-5\n",
    "    else:\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8a9a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d6f8a9a5",
    "outputId": "503483ec-e13f-4de2-ee40-9e602ca50b41",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14199 - L_r:0.001000  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 14299 - L_r:0.001000  loss_int : 0.00 - REN : 0.04 %\n",
      "Epoch 14399 - L_r:0.001000  loss_int : 0.01 - REN : 0.06 %\n",
      "Epoch 14499 - L_r:0.001000  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 14599 - L_r:0.001000  loss_int : 0.02 - REN : 0.24 %\n",
      "Epoch 14699 - L_r:0.001000  loss_int : 0.00 - REN : 0.05 %\n",
      "Epoch 14799 - L_r:0.001000  loss_int : 0.00 - REN : 0.04 %\n",
      "Epoch 14899 - L_r:0.001000  loss_int : 0.02 - REN : 0.12 %\n",
      "Epoch 14999 - L_r:0.001000  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 15099 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 15199 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 15299 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 15399 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 15499 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 15599 - L_r:0.001000  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 15699 - L_r:0.001000  loss_int : 0.01 - REN : 0.06 %\n",
      "Epoch 15799 - L_r:0.001000  loss_int : 0.00 - REN : 0.07 %\n",
      "Epoch 15899 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 15999 - L_r:0.001000  loss_int : 0.04 - REN : 0.33 %\n",
      "Epoch 16099 - L_r:0.001000  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 16199 - L_r:0.001000  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 16299 - L_r:0.001000  loss_int : 0.00 - REN : 0.04 %\n",
      "Epoch 16399 - L_r:0.001000  loss_int : 0.01 - REN : 0.21 %\n",
      "Epoch 16499 - L_r:0.001000  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 16599 - L_r:0.001000  loss_int : 0.00 - REN : 0.04 %\n",
      "Epoch 16699 - L_r:0.001000  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 16799 - L_r:0.001000  loss_int : 0.00 - REN : 0.06 %\n",
      "Epoch 16899 - L_r:0.001000  loss_int : 0.00 - REN : 0.04 %\n",
      "Epoch 16999 - L_r:0.001000  loss_int : 0.00 - REN : 0.07 %\n",
      "Epoch 17099 - L_r:0.001000  loss_int : 0.02 - REN : 0.16 %\n",
      "Epoch 17199 - L_r:0.001000  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 17299 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 17399 - L_r:0.001000  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 17499 - L_r:0.001000  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 17599 - L_r:0.001000  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 17699 - L_r:0.001000  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 17799 - L_r:0.001000  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 17899 - L_r:0.001000  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 17999 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 18099 - L_r:0.001000  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 18199 - L_r:0.001000  loss_int : 0.03 - REN : 0.14 %\n",
      "Epoch 18299 - L_r:0.001000  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 18399 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 18499 - L_r:0.001000  loss_int : 0.01 - REN : 0.09 %\n",
      "Epoch 18599 - L_r:0.001000  loss_int : 0.02 - REN : 0.14 %\n",
      "Epoch 18699 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 18799 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 18899 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 18999 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 19099 - L_r:0.001000  loss_int : 0.03 - REN : 0.15 %\n",
      "Epoch 19199 - L_r:0.001000  loss_int : 0.00 - REN : 0.06 %\n",
      "Epoch 19299 - L_r:0.001000  loss_int : 0.00 - REN : 0.04 %\n",
      "Epoch 19399 - L_r:0.001000  loss_int : 0.00 - REN : 0.05 %\n",
      "Epoch 19499 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 19599 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 19699 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 19799 - L_r:0.001000  loss_int : 0.00 - REN : 0.06 %\n",
      "Epoch 19899 - L_r:0.001000  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 19999 - L_r:0.001000  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 20099 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 20199 - L_r:0.000500  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 20299 - L_r:0.000500  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 20399 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 20499 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 20599 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 20699 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 20799 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 20899 - L_r:0.000500  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 20999 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 21099 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 21199 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 21299 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 21399 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 21499 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 21599 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 21699 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 21799 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 21899 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 21999 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 22099 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 22199 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 22299 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 22399 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 22499 - L_r:0.000500  loss_int : 0.01 - REN : 0.12 %\n",
      "Epoch 22599 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 22699 - L_r:0.000500  loss_int : 0.00 - REN : 0.04 %\n",
      "Epoch 22799 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 22899 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 22999 - L_r:0.000500  loss_int : 0.00 - REN : 0.06 %\n",
      "Epoch 23099 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 23199 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 23299 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 23399 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 23499 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 23599 - L_r:0.000500  loss_int : 0.00 - REN : 0.07 %\n",
      "Epoch 23699 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 23799 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 23899 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 23999 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 24099 - L_r:0.000500  loss_int : 0.01 - REN : 0.16 %\n",
      "Epoch 24199 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 24299 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 24399 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 24499 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 24599 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 24699 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 24799 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 24899 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 24999 - L_r:0.000500  loss_int : 0.00 - REN : 0.04 %\n",
      "Epoch 25099 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 25199 - L_r:0.000500  loss_int : 0.01 - REN : 0.18 %\n",
      "Epoch 25299 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 25399 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 25499 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 25599 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 25699 - L_r:0.000500  loss_int : 0.00 - REN : 0.05 %\n",
      "Epoch 25799 - L_r:0.000500  loss_int : 0.00 - REN : 0.06 %\n",
      "Epoch 25899 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 25999 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 26099 - L_r:0.000500  loss_int : 0.00 - REN : 0.04 %\n",
      "Epoch 26199 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 26299 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 26399 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 26499 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 26599 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 26699 - L_r:0.000500  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 26799 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 26899 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 26999 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 27099 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 27199 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 27299 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 27399 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 27499 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 27599 - L_r:0.000500  loss_int : 0.00 - REN : 0.06 %\n",
      "Epoch 27699 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 27799 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 27899 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 27999 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28099 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 28199 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 28299 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 28399 - L_r:0.000500  loss_int : 0.00 - REN : 0.07 %\n",
      "Epoch 28499 - L_r:0.000500  loss_int : 0.00 - REN : 0.03 %\n",
      "Epoch 28599 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 28699 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 28799 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 28899 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 28999 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 29099 - L_r:0.000500  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 29199 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 29299 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 29399 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 29499 - L_r:0.000500  loss_int : 0.00 - REN : 0.06 %\n",
      "Epoch 29599 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 29699 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 29799 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 29899 - L_r:0.000500  loss_int : 0.00 - REN : 0.05 %\n",
      "Epoch 29999 - L_r:0.000500  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 30099 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 30199 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 30299 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 30399 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 30499 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 30599 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 30699 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 30799 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 30899 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 30999 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 31099 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 31199 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 31299 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 31399 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 31499 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 31599 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 31699 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 31799 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 31899 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 31999 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 32099 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 32199 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 32299 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 32399 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 32499 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 32599 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 32699 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 32799 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 32899 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 32999 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 33099 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 33199 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 33299 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 33399 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 33499 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 33599 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 33699 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 33799 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 33899 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 33999 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 34099 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 34199 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 34299 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 34399 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 34499 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 34599 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 34699 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 34799 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 34899 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 34999 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 35099 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 35199 - L_r:0.000100  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 35299 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 35399 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 35499 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 35599 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 35699 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 35799 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 35899 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 35999 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 36099 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 36199 - L_r:0.000100  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 36299 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 36399 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 36499 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 36599 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 36699 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 36799 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 36899 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 36999 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 37099 - L_r:0.000100  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 37199 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 37299 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 37399 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 37499 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 37599 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 37699 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 37799 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 37899 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 37999 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 38099 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 38199 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 38299 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 38399 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 38499 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 38599 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 38699 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 38799 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 38899 - L_r:0.000100  loss_int : 0.00 - REN : 0.02 %\n",
      "Epoch 38999 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 39099 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 39199 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 39299 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 39399 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 39499 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 39599 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 39699 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 39799 - L_r:0.000100  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 39899 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 39999 - L_r:0.000100  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 40099 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 40199 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 40299 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 40399 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 40499 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 40599 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 40699 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 40799 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 40899 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 40999 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 41099 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 41199 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 41299 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 41399 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 41499 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 41599 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 41699 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 41799 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 41899 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41999 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 42099 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 42199 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 42299 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 42399 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 42499 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 42599 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 42699 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 42799 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 42899 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 42999 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 43099 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 43199 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 43299 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 43399 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 43499 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 43599 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 43699 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 43799 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 43899 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 43999 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 44099 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 44199 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 44299 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 44399 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 44499 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 44599 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 44699 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 44799 - L_r:0.000050  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 44899 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 44999 - L_r:0.000050  loss_int : 0.00 - REN : 0.01 %\n",
      "Epoch 45099 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 45199 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 45299 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 45399 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 45499 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 45599 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 45699 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 45799 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 45899 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 45999 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46099 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46199 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46299 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46399 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46499 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46599 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46699 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46799 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46899 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 46999 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47099 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47199 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47299 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47399 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47499 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47599 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47699 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47799 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47899 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 47999 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48099 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48199 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48299 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48399 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48499 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48599 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48699 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48799 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48899 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 48999 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49099 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49199 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49299 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49399 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49499 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49599 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49699 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49799 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49899 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n",
      "Epoch 49999 - L_r:0.000010  loss_int : 0.00 - REN : 0.00 %\n"
     ]
    }
   ],
   "source": [
    "#________________________ Training __________________________________________________________________________\n",
    "\n",
    "u = DGM_Net(dim,M,L).to(device)   #  The network that will approach the solution\n",
    "\n",
    "\n",
    "x_sampler = torch.distributions.uniform.Uniform(x_low, x_high)    #   to obtain the spatial data\n",
    "\n",
    "# Create the optimizer and a LambdaLR scheduler with the defined learning rate schedule\n",
    "optimizer = optim.Adam(u.parameters(),lr = 1)                   # Define the optimizer\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)  # to adjust learning rate\n",
    "\n",
    "\n",
    "loss_train = np.zeros(num_eps)             # loss of taining inside the domaine\n",
    "relative_E_losses = np.zeros(num_eps)              # losses initialization\n",
    "\n",
    "print('\\n Using {} device'.format(device))\n",
    "for ep in range(num_eps):\n",
    "\n",
    "\n",
    "    x = x_sampler.sample((mini_batch_size, dim))\n",
    "    x_bndr = x_sampler.sample((mini_batch_size_bdry, 1))\n",
    "\n",
    "    x = torch.Tensor(x).to(device)\n",
    "    x.requires_grad_()\n",
    "\n",
    "    # evaluate forward pass, compute derivatives of network with respect to x\n",
    "    u_output = u(x)\n",
    "    u_grad = torch.autograd.grad(u_output, x, grad_outputs=torch.ones_like(u_output), create_graph=True)[0]\n",
    "\n",
    "    Δu = 0.0\n",
    "    for i in range(dim):\n",
    "        Δu += (torch.autograd.grad(u_grad[:,i], x, grad_outputs=torch.ones_like(u_grad[:,i]), create_graph=True)[0][:,i]).reshape(-1,1)\n",
    "\n",
    "    L1 = area * torch.mean(torch.pow((-Δu + pi**2 *u_output  - f(x)), 2))\n",
    "\n",
    "    x_bry_1 = torch.cat((zeros,x_bndr),1).to(device)\n",
    "\n",
    "    x_bry_1.requires_grad_()\n",
    "    u_bry_1 = u(x_bry_1)\n",
    "    u_grad_bry_1 = torch.autograd.grad(u_bry_1, x_bry_1, grad_outputs=torch.ones_like(u_bry_1), create_graph=True)[0][:,0]\n",
    "    L2 = torch.mean(torch.pow(u_grad_bry_1, 2))\n",
    "\n",
    "    x_bry_2 = torch.cat((ones,x_bndr),1).to(device)\n",
    "    x_bry_2.requires_grad_()\n",
    "    u_bry_2 = u(x_bry_2)\n",
    "    u_grad_bry_2 = torch.autograd.grad(u_bry_2, x_bry_2, grad_outputs=torch.ones_like(u_bry_2), create_graph=True)[0][:,0]\n",
    "    L3 = torch.mean(torch.pow(u_grad_bry_2, 2))\n",
    "\n",
    "    y_bry_1 = torch.cat((x_bndr,zeros),1).to(device)\n",
    "    y_bry_1.requires_grad_()\n",
    "    u_bry_3 = u(y_bry_1)\n",
    "    u_grad_bry_3 = torch.autograd.grad(u_bry_3, y_bry_1, grad_outputs=torch.ones_like(u_bry_3), create_graph=True)[0][:,1]\n",
    "    L4 = torch.mean(torch.pow(u_grad_bry_3, 2))\n",
    "\n",
    "    y_bry_2 = torch.cat((x_bndr,ones),1).to(device)\n",
    "    y_bry_2.requires_grad_()\n",
    "    u_bry_4 = u(y_bry_2)\n",
    "    u_grad_bry_4 = torch.autograd.grad(u_bry_4, y_bry_2, grad_outputs=torch.ones_like(u_bry_4), create_graph=True)[0][:,1]\n",
    "    L5 = torch.mean(torch.pow(u_grad_bry_4, 2))\n",
    "\n",
    "\n",
    "\n",
    "    Loss = L1 + L2 + L3 + L4 + L5\n",
    "\n",
    "    relative_E_losses[ep] =  float(relative_E( U_exat(x) ,u_output).item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    Loss.backward()            # compute derivative of loss with respect to network parameters\n",
    "    optimizer.step()           # update network parameters with ADAM\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step(ep)  # Pass the current epoch to the scheduler\n",
    "\n",
    "    # Display the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    loss_train[ep] = float(L1.item())\n",
    "    if ep % 100 == 99:\n",
    "        print(\"Epoch %d - L_r:%f  loss_int : %.2f - REN : %.2f\"%(ep, current_lr, loss_train[ep], relative_E_losses[ep]),\"%\")\n",
    "        #Ploting(x,x_bndr, u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mfI2JPN2l-Gi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfI2JPN2l-Gi",
    "outputId": "cbfdf1ff-477e-49e2-b718-cfa9a0110871"
   },
   "outputs": [],
   "source": [
    "\n",
    "#_______________________________________________________________________________\n",
    "\n",
    "grid_nums = 500\n",
    "x_test = torch.distributions.uniform.Uniform(x_low, x_high).sample((grid_nums, dim)).to(device)\n",
    "with torch.no_grad():\n",
    "    U = u(x_test).detach()\n",
    "\n",
    "U_ext =  U_exat(x_test)\n",
    "\n",
    "print(\"The Mean Absulat Error: {}\".format(absulat_ME(U_ext,U)))\n",
    "print(\"The Relative Error: {} %\".format(relative_E(U_ext,U)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842bbad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5842bbad",
    "outputId": "f2f79ed4-ce61-4884-ca35-ac29f68ab74c"
   },
   "outputs": [],
   "source": [
    "MSEabs = 0\n",
    "MSErlf = 0\n",
    "N = 100\n",
    "for compt in range(N):\n",
    "    x_test = torch.distributions.uniform.Uniform(x_low, x_high).sample((grid_nums, dim)).to(device)\n",
    "    U = u(x_test)\n",
    "    U_ext =  U_exat(x_test)\n",
    "\n",
    "    MSEabs+= absulat_ME(U_ext,U)\n",
    "    MSErlf+= relative_E(U_ext,U)\n",
    "\n",
    "print(\"The averag of Mean Absulat Error: {}\".format(MSEabs/N))\n",
    "print(\"The averag of Relative Error: {} %\".format(MSErlf/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m1DmFmqkl97R",
   "metadata": {
    "id": "m1DmFmqkl97R"
   },
   "outputs": [],
   "source": [
    "# #_______________________ Save Model _____________________________________\n",
    "\n",
    "# torch.save(u.state_dict(), \"Model_DGM_Problem_with_less_regular_solution_1000\") # to save weights\n",
    "# np.save(\"loss_DGM_Problem_with_less_regular_solution_1000\" ,Losse)                # to save Losse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f78d595",
   "metadata": {
    "id": "0f78d595"
   },
   "outputs": [],
   "source": [
    "# # # # #_______________________ load Model _____________________________________\n",
    "\n",
    "# u_IS = DGM_Net(dim,M,L).to(device)\n",
    "# state_dict = torch.load( \"Model_IS_DGM_Poisson_special_50000_2.pth\")\n",
    "# u_IS.load_state_dict(state_dict)\n",
    "# losse_IS = np.load(\"loss_IS_DGM_Poisson_special_50000_2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee5a6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "79ee5a6b",
    "outputId": "a913c656-b400-439d-ab1c-76778d052bce"
   },
   "outputs": [],
   "source": [
    "# # #_______________ Ploting results __________________________________\n",
    "\n",
    "\n",
    "# grid_nums = 50\n",
    "\n",
    "# x_grid = torch.linspace(-1.0,1.0,grid_nums)\n",
    "# XX, YY = torch.meshgrid(x_grid,x_grid)\n",
    "# X = torch.reshape(XX, (-1,1))\n",
    "# Y = torch.reshape(YY, (-1,1))\n",
    "# XY = torch.cat((X,Y),1).to(device)\n",
    "# with torch.no_grad():\n",
    "#     U = u_IS(XY)\n",
    "\n",
    "# UU = torch.reshape(U, (grid_nums,grid_nums)).detach().cpu().numpy()\n",
    "# U_ext =  U_exat(XY)\n",
    "# U_ext = torch.reshape(U_ext , (grid_nums,grid_nums)).detach().cpu().numpy()\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "# ax.plot_surface(XX.numpy(),YY.numpy(), UU )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4Ja9wZq-_8Mp",
   "metadata": {
    "id": "4Ja9wZq-_8Mp",
    "outputId": "c7a04b7c-9ffd-4a29-cf60-0211c78cd0ff"
   },
   "outputs": [],
   "source": [
    "# grid_nums = 500\n",
    "# x_test = torch.distributions.uniform.Uniform(x_low, x_high).sample((grid_nums, dim)).to(device)\n",
    "# with torch.no_grad():\n",
    "#     U = u(x_test).detach()\n",
    "#     U_IS = u_IS(x_test).detach()\n",
    "\n",
    "\n",
    "# U_ext =  U_exat(x_test)\n",
    "\n",
    "# print(\"MAE_standard_DGM = {} | MAE_Proposed method = {}\".format(absulat_ME(U_ext,U), absulat_ME(U_ext,U_IS)))\n",
    "\n",
    "# print(\"MRE_standard_DGM = {}% | MRE_Proposed method = {}%\".format(relative_E(U_ext,U), relative_E(U_ext,U_IS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DFNa7oXWCBV0",
   "metadata": {
    "id": "DFNa7oXWCBV0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab6f89",
   "metadata": {
    "id": "07ab6f89",
    "outputId": "85157d18-ec5e-47c3-cf64-51aa6114bb9e"
   },
   "outputs": [],
   "source": [
    "# Losse_IS  = np.log(losse_IS)\n",
    "# Losse_standard  = np.log(Losse)\n",
    "\n",
    "# x_error = np.linspace(0,50000, 50000).reshape(-1,1)\n",
    "# plt.plot(x_error, Losse_IS,  label='Losse_IS')\n",
    "# plt.plot(x_error, Losse_standard, label='Losse_standard')\n",
    "# plt.legend()\n",
    "# plt.show"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
